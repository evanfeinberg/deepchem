{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Evan Feinberg and Bharath Ramsundar\n",
    "\n",
    "Copyright 2016, Stanford University\n",
    "\n",
    "#Welcome to the deepchem tutorial. In this iPython Notebook, one can follow along with the code below to learn how to fit machine learning models with rich predictive power on chemical datasets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview:\n",
    "\n",
    "In this tutorial, you will trace an arc from loading a raw dataset to fitting a cutting edge ML technique for predicting binding affinities. This will be accomplished by writing simple commands to access the deepchem Python API, encompassing the following broad steps:\n",
    "\n",
    "1. Loading a chemical dataset, consisting of a series of protein-ligand complexes.\n",
    "2. Featurizing each protein-ligand complexes with various featurization schemes. \n",
    "3. Fitting a series of models with these featurized protein-ligand complexes.\n",
    "4. Visualizing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's point to a \"dataset\" file. This can come in the format of a CSV file or Pandas DataFrame. Regardless\n",
    "of file format, it must be columnar data, where each row is a molecular system, and each column represents\n",
    "a different piece of information about that system. For instance, in this example, every row reflects a \n",
    "protein-ligand complex, and the following columns are present: a unique complex identifier; the SMILES string\n",
    "of the ligand; the binding affinity (Ki) of the ligand to the protein in the complex; a Python `list` of all lines\n",
    "in a PDB file for the protein alone; and a Python `list` of all lines in a ligand file for the ligand alone.\n",
    "\n",
    "This should become clearer with the example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_file= \"../datasets/pdbbind_core_df.pkl.gz\"\n",
    "from deepchem.utils.save import load_from_disk\n",
    "dataset = load_from_disk(dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what `dataset` looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of dataset is: <class 'pandas.core.frame.DataFrame'>\n",
      "Columns of dataset are: ['pdb_id' 'smiles' 'complex_id' 'protein_pdb' 'ligand_pdb' 'ligand_mol2'\n",
      " 'label']\n",
      "Shape of dataset is: (193, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of dataset is: %s\" % str(type(dataset)))\n",
    "print(\"Columns of dataset are: %s\" % str(dataset.columns.values))\n",
    "print(\"Shape of dataset is: %s\" % str(dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's take a quick look at the first molecule. The first complex contains a protein with a PDB ID of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2d3u'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_1 = dataset.iterrows().next()[1]\n",
    "complex_1[\"pdb_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", and contains a ligand with a SMILES string of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CC1CCCCC1S(O)(O)NC1CC(C2CCC(CN)CC2)SC1C(O)O'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_1[\"smiles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This complex has a `Ki` of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.92'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_1['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're oriented, let's use ML to do some chemistry. \n",
    "\n",
    "So, step (2) will entail featurizing the dataset.\n",
    "\n",
    "The available featurizations that come standard with deepchem are ECFP4 fingerprints, RDKit descriptors, and NNScore binding pocket descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.featurizers.fingerprints import CircularFingerprint\n",
    "from deepchem.featurizers.basic import RDKitDescriptors\n",
    "from deepchem.featurizers.nnscore import NNScoreComplexFeaturizer\n",
    "\n",
    "compound_featurizers = [CircularFingerprint(size=1024)]\n",
    "complex_featurizers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we separate our featurizers into those that featurize individual chemical compounds, compound_featurizers, and those that featurize molecular complexes, complex_featurizers.\n",
    "\n",
    "Now, let's perform the actual featurization. Calling ```featurizer.featurize()``` will return an instance of class ```FeaturizedSamples```. Internally, ```featurizer.featurize()``` (a) computes the user-specified features on the data, (b) transforms the inputs into X and y NumPy arrays suitable for ML algorithms, and (c) constructs a ```FeaturizedSamples()``` instance that has useful methods, such as an iterator, over the featurized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make a directory in which to store the featurized complexes.\n",
    "import tempfile, shutil\n",
    "feature_dir = tempfile.mkdtemp()\n",
    "samples_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.featurizers.featurize import DataFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featurizers = compound_featurizers + complex_featurizers\n",
    "featurizer = DataFeaturizer(tasks=[\"label\"],\n",
    "                            smiles_field=\"smiles\",\n",
    "                            protein_pdb_field=\"protein_pdb\",\n",
    "                            ligand_pdb_field=\"ligand_pdb\",\n",
    "                            compound_featurizers=compound_featurizers,\n",
    "                            complex_featurizers=complex_featurizers,\n",
    "                            id_field=\"complex_id\",\n",
    "                            verbose=False)\n",
    "featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,\n",
    "                                          shard_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we conduct a train-test split. If you'd like, you can choose `splittype=\"scaffold\"` instead to perform a train-test split based on Bemis-Murcko scaffolds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splittype = \"random\"\n",
    "train_dir, test_dir = tempfile.mkdtemp(), tempfile.mkdtemp()\n",
    "\n",
    "\n",
    "train_samples, test_samples = featurized_samples.train_test_split(\n",
    "    splittype, train_dir, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate separate instances of the Dataset() object to hermetically seal the train dataset from the test dataset. This style lends itself easily to validation-set type hyperparameter searches, which we will illustate in a separate tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.utils.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(data_dir=train_dir, samples=train_samples, \n",
    "                        featurizers=featurizers, tasks=[\"label\"])\n",
    "test_dataset = Dataset(data_dir=test_dir, samples=test_samples, \n",
    "                       featurizers=featurizers, tasks=[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of many ML algorithms hinges greatly on careful data preprocessing. Deepchem comes standard with a few options for such preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_transforms = [\"normalize\", \"truncate\"]\n",
    "output_transforms = [\"normalize\"]\n",
    "train_dataset.transform(input_transforms, output_transforms)\n",
    "test_dataset.transform(input_transforms, output_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to do some learning! To set up a model, we will need: (a) a dictionary ```task_types``` that maps a task, in this case ```label```, i.e. the Ki, to the type of the task, in this case ```regression```. For the multitask use case, one will have a series of keys, each of which is a different task (Ki, solubility, renal half-life, etc.) that maps to a different task type (regression or classification).\n",
    "\n",
    "To fit a deepchem model, first we instantiate one of the provided (or user-written) model classes. In this case, we have a created a convenience class to wrap around any ML model available in Sci-Kit Learn that can in turn be used to interoperate with deepchem. To instantiate an ```SklearnModel```, you will need (a) task_types, (b) model_params, another ```dict``` as illustrated below, and (c) a ```model_instance``` defining the type of model you would like to fit, in this case a ```RandomForestRegressor```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from deepchem.models.standard import SklearnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task_types = {\"label\": \"regression\"}\n",
    "model_params = {\"data_shape\": train_dataset.get_data_shape()}\n",
    "\n",
    "model = SklearnModel(task_types, model_params, model_instance=RandomForestRegressor())\n",
    "model.fit(train_dataset)\n",
    "model_dir = tempfile.mkdtemp()\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.utils.evaluate import Evaluator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f10c4615930>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f10c4615150>\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f10c4615660>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f10c4615540>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>r2_score</th>\n",
       "      <th>rms_error</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>0.796008</td>\n",
       "      <td>1.007532</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>-0.141560</td>\n",
       "      <td>2.453933</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_name  r2_score  rms_error  split\n",
       "0     label  0.796008   1.007532  train\n",
       "0     label -0.141560   2.453933   test"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = Evaluator(model, train_dataset, verbose=True)\n",
    "with tempfile.NamedTemporaryFile() as train_csv_out:\n",
    "  with tempfile.NamedTemporaryFile() as train_stats_out:\n",
    "    _, train_r2score = evaluator.compute_model_performance(\n",
    "        train_csv_out, train_stats_out)\n",
    "\n",
    "evaluator = Evaluator(model, test_dataset, verbose=True)\n",
    "with tempfile.NamedTemporaryFile() as test_csv_out:\n",
    "  with tempfile.NamedTemporaryFile() as test_stats_out:\n",
    "    _, test_r2score = evaluator.compute_model_performance(\n",
    "        test_csv_out, test_stats_out)\n",
    "\n",
    "train_test_performance = pd.concat([train_r2score, test_r2score])\n",
    "train_test_performance[\"split\"] = [\"train\", \"test\"]\n",
    "train_test_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple example, in few yet intuitive lines of code, traces the machine learning arc from featurizing a raw dataset to fitting and evaluating a model. \n",
    "\n",
    "In the next section, we illustrate ```deepchem```'s modularity, and thereby the ease with which one can explore different featurization schemes, different models, and combinations thereof, to achieve the best performance on a given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class fingerprints32(CircularFingerprint):\n",
    "    def __init__(self):\n",
    "        super(fingerprints32, self).__init__(size=32)\n",
    "        \n",
    "class fingerprints64(CircularFingerprint):\n",
    "    def __init__(self):\n",
    "        super(fingerprints64, self).__init__(size=64)\n",
    "        \n",
    "class fingerprints128(CircularFingerprint):\n",
    "    def __init__(self):\n",
    "        super(fingerprints128, self).__init__(size=128)\n",
    "\n",
    "class fingerprints256(CircularFingerprint):\n",
    "    def __init__(self):\n",
    "        super(fingerprints256, self).__init__(size=256)\n",
    "\n",
    "class fingerprints512(CircularFingerprint):\n",
    "    def __init__(self):\n",
    "        super(fingerprints512, self).__init__(size=512)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done featurizing.\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f969cc00300>done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f969cc004b0>done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.done featurizing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done featurizing.done featurizing.\n",
      "\n",
      "done featurizing.done featurizing.\n",
      "\n",
      "done featurizing.\n",
      "done featurizing.\n",
      "done featurizing.\n",
      "done featurizing.\n",
      "\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f96b99a5f60>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f96b9f0a030>\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f96b99a5810>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f969cc00300>\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f969cc004b0>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f96b99a5f60>\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f96b9f0a030>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f96b99a5810>\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f969cc00300>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f969cc004b0>\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f96b99a5f60>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f96b9f0a030>\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f96b9f0a810>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f969cc00300>\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f969cc004b0>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f96b99a5f60>\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f96b9f0a030>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f96b9f0a810>\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f969cc00300>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f969cc004b0>\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f96b99a5f60>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f96b9f0a030>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>r2_score</th>\n",
       "      <th>rms_error</th>\n",
       "      <th>split</th>\n",
       "      <th>featurizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>0.813750</td>\n",
       "      <td>0.952139</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;class '__main__.fingerprints32'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>-0.226778</td>\n",
       "      <td>2.465132</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;class '__main__.fingerprints32'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>0.831418</td>\n",
       "      <td>0.905853</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;class '__main__.fingerprints64'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>0.159377</td>\n",
       "      <td>2.040600</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;class '__main__.fingerprints64'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>0.828895</td>\n",
       "      <td>0.912606</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;class '__main__.fingerprints128'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>-0.049061</td>\n",
       "      <td>2.279595</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;class '__main__.fingerprints128'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>0.786390</td>\n",
       "      <td>1.019678</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;class '__main__.fingerprints256'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>-0.139194</td>\n",
       "      <td>2.375507</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;class '__main__.fingerprints256'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>0.808448</td>\n",
       "      <td>0.965595</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;class '__main__.fingerprints512'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>0.058960</td>\n",
       "      <td>2.159044</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;class '__main__.fingerprints512'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>2.206310</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;class 'deepchem.featurizers.grid_featurizer.G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>2.225727</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;class 'deepchem.featurizers.grid_featurizer.G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_name  r2_score  rms_error  split  \\\n",
       "0     label  0.813750   0.952139  train   \n",
       "0     label -0.226778   2.465132   test   \n",
       "0     label  0.831418   0.905853  train   \n",
       "0     label  0.159377   2.040600   test   \n",
       "0     label  0.828895   0.912606  train   \n",
       "0     label -0.049061   2.279595   test   \n",
       "0     label  0.786390   1.019678  train   \n",
       "0     label -0.139194   2.375507   test   \n",
       "0     label  0.808448   0.965595  train   \n",
       "0     label  0.058960   2.159044   test   \n",
       "0     label -0.000068   2.206310  train   \n",
       "0     label -0.000068   2.225727   test   \n",
       "\n",
       "                                          featurizer  \n",
       "0                  <class '__main__.fingerprints32'>  \n",
       "0                  <class '__main__.fingerprints32'>  \n",
       "0                  <class '__main__.fingerprints64'>  \n",
       "0                  <class '__main__.fingerprints64'>  \n",
       "0                 <class '__main__.fingerprints128'>  \n",
       "0                 <class '__main__.fingerprints128'>  \n",
       "0                 <class '__main__.fingerprints256'>  \n",
       "0                 <class '__main__.fingerprints256'>  \n",
       "0                 <class '__main__.fingerprints512'>  \n",
       "0                 <class '__main__.fingerprints512'>  \n",
       "0  <class 'deepchem.featurizers.grid_featurizer.G...  \n",
       "0  <class 'deepchem.featurizers.grid_featurizer.G...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imp import reload\n",
    "import deepchem.featurizers.grid_featurizer\n",
    "reload(deepchem.featurizers.grid_featurizer)\n",
    "from deepchem.featurizers.grid_featurizer import GridFeaturizer \n",
    "compound_featurizers = [fingerprints32(), fingerprints64(), fingerprints128(), fingerprints256(), fingerprints512()]\n",
    "complex_featurizers = [GridFeaturizer(voxel_width=16.0, feature_types=\"voxel_combined\", voxel_feature_types=[\"ecfp\",\n",
    "                                      \"splif\", \"hbond\", \"pi_stack\", \"cation_pi\", \"salt_bridge\"], ecfp_power=5, splif_power=5,\n",
    "                                     parallel=True, flatten=True)]\n",
    "\n",
    "featurizers = compound_featurizers + complex_featurizers\n",
    "featurizer = DataFeaturizer(tasks=[\"label\"],\n",
    "                            smiles_field=\"smiles\",\n",
    "                            protein_pdb_field=\"protein_pdb\",\n",
    "                            ligand_pdb_field=\"ligand_pdb\",\n",
    "                            compound_featurizers=compound_featurizers,\n",
    "                            complex_featurizers=complex_featurizers,\n",
    "                            id_field=\"complex_id\",\n",
    "                            verbose=False)\n",
    "featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,\n",
    "                                          shard_size=100)\n",
    "print(\"Done featurizing.\")\n",
    "train_dir, test_dir = tempfile.mkdtemp(), tempfile.mkdtemp()\n",
    "splittype=\"scaffold\"\n",
    "train_samples, test_samples = featurized_samples.train_test_split(\n",
    "    splittype, train_dir, test_dir)\n",
    "\n",
    "task_types = {\"label\": \"regression\"}\n",
    "\n",
    "\n",
    "performance = pd.DataFrame()\n",
    "all_featurizers = compound_featurizers + complex_featurizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pdb_id                                             smiles  \\\n",
      "0   2d3u        CC1CCCCC1S(O)(O)NC1CC(C2CCC(CN)CC2)SC1C(O)O   \n",
      "1   3cyx  CC(C)(C)NC(O)C1CC2CCCCC2C[NH+]1CC(O)C(CC1CCCCC...   \n",
      "2   3uo4        OC(O)C1CCC(NC2NCCC(NC3CCCCC3C3CCCCC3)N2)CC1   \n",
      "3   1p1q                         CC1ONC(O)C1CC([NH3+])C(O)O   \n",
      "4   3ag9  NC(O)C(CCC[NH2+]C([NH3+])[NH3+])NC(O)C(CCC[NH2...   \n",
      "\n",
      "                                          complex_id  \\\n",
      "0    2d3uCC1CCCCC1S(O)(O)NC1CC(C2CCC(CN)CC2)SC1C(O)O   \n",
      "1  3cyxCC(C)(C)NC(O)C1CC2CCCCC2C[NH+]1CC(O)C(CC1C...   \n",
      "2    3uo4OC(O)C1CCC(NC2NCCC(NC3CCCCC3C3CCCCC3)N2)CC1   \n",
      "3                     1p1qCC1ONC(O)C1CC([NH3+])C(O)O   \n",
      "4  3ag9NC(O)C(CCC[NH2+]C([NH3+])[NH3+])NC(O)C(CCC...   \n",
      "\n",
      "                                         protein_pdb  \\\n",
      "0  [HEADER    2D3U PROTEIN\\n, COMPND    2D3U PROT...   \n",
      "1  [HEADER    3CYX PROTEIN\\n, COMPND    3CYX PROT...   \n",
      "2  [HEADER    3UO4 PROTEIN\\n, COMPND    3UO4 PROT...   \n",
      "3  [HEADER    1P1Q PROTEIN\\n, COMPND    1P1Q PROT...   \n",
      "4  [HEADER    3AG9 PROTEIN\\n, COMPND    3AG9 PROT...   \n",
      "\n",
      "                                          ligand_pdb  \\\n",
      "0  [COMPND    2d3u ligand \\n, AUTHOR    GENERATED...   \n",
      "1  [COMPND    3cyx ligand \\n, AUTHOR    GENERATED...   \n",
      "2  [COMPND    3uo4 ligand \\n, AUTHOR    GENERATED...   \n",
      "3  [COMPND    1p1q ligand \\n, AUTHOR    GENERATED...   \n",
      "4  [COMPND    3ag9 ligand \\n, AUTHOR    GENERATED...   \n",
      "\n",
      "                                         ligand_mol2 label  \n",
      "0  [### \\n, ### Created by X-TOOL on Thu Aug 28 2...  6.92  \n",
      "1  [### \\n, ### Created by X-TOOL on Thu Aug 28 2...  8.00  \n",
      "2  [### \\n, ### Created by X-TOOL on Fri Aug 29 0...  6.52  \n",
      "3  [### \\n, ### Created by X-TOOL on Thu Aug 28 2...  4.89  \n",
      "4  [### \\n, ### Created by X-TOOL on Thu Aug 28 2...  8.05  \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridFeaturizer' object has no attribute '_featurize_complexes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-667da7dcef93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m                             verbose=False)\n\u001b[0;32m     30\u001b[0m featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,\n\u001b[1;32m---> 31\u001b[1;33m                                           shard_size=100)\n\u001b[0m",
      "\u001b[1;32m/scratch/users/enf/deep-docking/deepchem/deepchem/featurizers/featurize.py\u001b[0m in \u001b[0;36mfeaturize\u001b[1;34m(self, input_file, feature_dir, samples_dir, shard_size)\u001b[0m\n\u001b[0;32m    155\u001b[0m         log(\"Currently feauturizing feature_type: %s\"\n\u001b[0;32m    156\u001b[0m             % complex_featurizer.__class__.__name__, self.verbose)\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_featurize_complexes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplex_featurizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m       \u001b[0mshard_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"features_shard%d.joblib\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/scratch/users/enf/deep-docking/deepchem/deepchem/featurizers/featurize.py\u001b[0m in \u001b[0;36m_featurize_complexes\u001b[1;34m(self, df, featurizer)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     features = ProcessingPool(mp.cpu_count()).map(featurize_wrapper, \n\u001b[1;32m--> 218\u001b[1;33m                                                   zip(ligand_pdbs, protein_pdbs))\n\u001b[0m\u001b[0;32m    219\u001b[0m     \u001b[1;31m#features = featurize_wrapper(zip(ligand_pdbs, protein_pdbs))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Final features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/enf/anaconda/lib/python2.7/site-packages/pathos/multiprocessing.pyc\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, f, *args, **kwds)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mAbstractWorkerPool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AbstractWorkerPool__map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0m_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_serve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# chunksize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[0mmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractWorkerPool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mimap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/enf/anaconda/lib/python2.7/site-packages/multiprocess/pool.pyc\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    249\u001b[0m         '''\n\u001b[0;32m    250\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mRUN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mimap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/enf/anaconda/lib/python2.7/site-packages/multiprocess/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    565\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridFeaturizer' object has no attribute '_featurize_complexes'"
     ]
    }
   ],
   "source": [
    "dataset_file= \"../datasets/pdbbind_core_5_df.pkl.gz\"\n",
    "from deepchem.utils.save import load_from_disk\n",
    "from deepchem.featurizers.featurize import DataFeaturizer\n",
    "dataset = load_from_disk(dataset_file)\n",
    "print(dataset)\n",
    "\n",
    "from imp import reload\n",
    "import deepchem.featurizers.grid_featurizer\n",
    "reload(deepchem.featurizers.grid_featurizer)\n",
    "from deepchem.featurizers.grid_featurizer import GridFeaturizer \n",
    "import deepchem.featurizers.featurize\n",
    "reload(deepchem.featurizers.featurize)\n",
    "from deepchem.featurizers.featurize import DataFeaturizer\n",
    "\n",
    "compound_featurizers = []\n",
    "complex_featurizers = [GridFeaturizer(voxel_width=16.0, feature_types=\"voxel_combined\", voxel_feature_types=[\"ecfp\",\n",
    "                                      \"splif\", \"hbond\", \"pi_stack\", \"cation_pi\", \"salt_bridge\"], ecfp_power=5, splif_power=5,\n",
    "                                     parallel=True, flatten=True)]\n",
    "\n",
    "feature_dir = tempfile.mkdtemp()\n",
    "featurizers = compound_featurizers + complex_featurizers\n",
    "featurizer = DataFeaturizer(tasks=[\"label\"],\n",
    "                            smiles_field=\"smiles\",\n",
    "                            protein_pdb_field=\"protein_pdb\",\n",
    "                            ligand_pdb_field=\"ligand_pdb\",\n",
    "                            compound_featurizers=compound_featurizers,\n",
    "                            complex_featurizers=complex_featurizers,\n",
    "                            id_field=\"complex_id\",\n",
    "                            verbose=False)\n",
    "featurized_samples = featurizer.featurize(dataset_file, feature_dir, samples_dir,\n",
    "                                          shard_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "(154, 8)\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f965faa99c0>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f96b99a5300>\n",
      "Saving predictions to <open file '<fdopen>', mode 'w+b' at 0x7f965faa9150>\n",
      "Saving model performance scores to <open file '<fdopen>', mode 'w+b' at 0x7f96acd07660>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_name</th>\n",
       "      <th>r2_score</th>\n",
       "      <th>rms_error</th>\n",
       "      <th>split</th>\n",
       "      <th>featurizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>-0.00005</td>\n",
       "      <td>2.206290</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;class 'deepchem.featurizers.grid_featurizer.G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>label</td>\n",
       "      <td>-0.00005</td>\n",
       "      <td>2.225708</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;class 'deepchem.featurizers.grid_featurizer.G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task_name  r2_score  rms_error  split  \\\n",
       "0     label  -0.00005   2.206290  train   \n",
       "0     label  -0.00005   2.225708   test   \n",
       "\n",
       "                                          featurizer  \n",
       "0  <class 'deepchem.featurizers.grid_featurizer.G...  \n",
       "0  <class 'deepchem.featurizers.grid_featurizer.G...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance = pd.DataFrame()\n",
    "import deepchem.models.standard\n",
    "reload(deepchem.models.standard)\n",
    "from deepchem.models.standard import SklearnModel\n",
    "\n",
    "for feature_type in complex_featurizers:\n",
    "    train_dataset = Dataset(data_dir=train_dir, samples=train_samples, \n",
    "                        featurizers=[feature_type], tasks=[\"label\"])\n",
    "    test_dataset = Dataset(data_dir=test_dir, samples=test_samples, \n",
    "                       featurizers=[feature_type], tasks=[\"label\"])\n",
    "\n",
    "\n",
    "    input_transforms = [\"normalize\", \"truncate\"]\n",
    "    output_transforms = [\"normalize\"]\n",
    "    train_dataset.transform(input_transforms, output_transforms)\n",
    "    test_dataset.transform(input_transforms, output_transforms)\n",
    "\n",
    "    model_params = {\"data_shape\": train_dataset.get_data_shape()}\n",
    "\n",
    "    model = SklearnModel(task_types, model_params, model_instance=RandomForestRegressor())\n",
    "    model.fit(train_dataset)\n",
    "    model_dir = tempfile.mkdtemp()\n",
    "    model.save(model_dir)\n",
    "\n",
    "\n",
    "    evaluator = Evaluator(model, train_dataset, verbose=True)\n",
    "    with tempfile.NamedTemporaryFile() as train_csv_out:\n",
    "      with tempfile.NamedTemporaryFile() as train_stats_out:\n",
    "        _, train_r2score = evaluator.compute_model_performance(\n",
    "            train_csv_out, train_stats_out)\n",
    "\n",
    "    evaluator = Evaluator(model, test_dataset, verbose=True)\n",
    "    with tempfile.NamedTemporaryFile() as test_csv_out:\n",
    "      with tempfile.NamedTemporaryFile() as test_stats_out:\n",
    "        _, test_r2score = evaluator.compute_model_performance(\n",
    "            test_csv_out, test_stats_out)\n",
    "\n",
    "    train_test_performance = pd.concat([train_r2score, test_r2score])\n",
    "    train_test_performance[\"split\"] = [\"train\", \"test\"]\n",
    "    train_test_performance[\"featurizer\"] = [feature_type.__class__, feature_type.__class__]\n",
    "    performance = pd.concat([performance, train_test_performance])\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have computed three separate featurizations -- ECFP4, RDKitDescriptors, and NNScore -- in this example we choose to use the RDKitDescriptors featurization. This will serve as a baseline as you apply more advanced featurization schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is essential for many ML methods, depending on the data type, to perform preprocessing in order to attain optimal performance. Here, we choose to normalize and truncate the inputs (X, the featurized complexes) while normalizing the output (y, the binding affinities):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, the initial dataset has now been featurized, split into train and test sets, and transformed. We are now ready to learn some chemistry! To warm up, let's apply a more traditional but quite robust statistical learning technique: random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fit an Random Forest Regressor on the data, let's evaluate its performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to performance with a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.models import Model\n",
    "\n",
    "task_type = \"regression\"\n",
    "model_params = {\"activation\": \"relu\",\n",
    "              \"dropout\": 0.,\n",
    "              \"momentum\": .9, \"nesterov\": False,\n",
    "              \"decay\": 1e-4, \"batch_size\": 5,\n",
    "              \"nb_epoch\": 10}\n",
    "model_name = \"singletask_deep_regressor\"\n",
    "\n",
    "nb_hidden_vals = [10, 100]\n",
    "learning_rate_vals = [.01, .001]\n",
    "init_vals = [\"glorot_uniform\"]\n",
    "hyperparameters = [nb_hidden_vals, learning_rate_vals, init_vals]\n",
    "hyperparameter_rows = []\n",
    "for hyperparameter_tuple in itertools.product(*hyperparameters):\n",
    "    nb_hidden, learning_rate, init = hyperparameter_tuple\n",
    "    model_params[\"nb_hidden\"] = nb_hidden\n",
    "    model_params[\"learning_rate\"] = learning_rate\n",
    "    model_params[\"init\"] = init\n",
    "\n",
    "    model_dir = tempfile.mkdtemp()\n",
    "\n",
    "    r2_score = create_and_eval_model(train_dataset, test_dataset, task_type,\n",
    "                                     model_params, model_name, model_dir, tasks)\n",
    "\n",
    "    print(\"%s: %s\" % (hyperparameter_tuple, r2_score))\n",
    "    hyperparameter_rows.append(list(hyperparameter_tuple) + [r2_score])\n",
    "\n",
    "    shutil.rmtree(model_dir)\n",
    "\n",
    "hyperparameter_df = pd.DataFrame(hyperparameter_rows,\n",
    "                               columns=('nb_hidden', 'learning_rate',\n",
    "                                        'init', 'r2_score'))\n",
    "\n",
    "hyperparameter_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
